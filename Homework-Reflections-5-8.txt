Homework Reflection 5

1. Draw a diagram for the following negative feedback loop:

Sweating causes body temperature to decrease.  High body temperature causes sweating.

A negative feedback loop means that one thing increases another while the second thing decreases the first.

Remember that we are using directed acyclic graphs where two things cannot directly cause each other.

Sweat(t) ---> Body temp(t+1)

Body temp(t) ---> Sweat(t+1)

2. Describe an example of a positive feedback loop.  This means that one things increases another while the second things also increases the first.

A positive feedback loop could be the following:

When a person practices a skill, they become more proficient at it. As they become more proficient, they are more likely to practice the skill again, leading to further improvement.

3. Draw a diagram for the following situation:

Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.
Bears eat deer, decreasing their population.
Deer eat flowers, decreasing their population.

            Lightning Storm --> Flowers
                   / \         ^
                  /   \       /
                 v     v     /  
            Bears ----> Deer

Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.

'''python
# Create a dataset from the above DAG chart
import numpy as np
import pandas as pd

num = 100 
lightning = 3 * np.random.uniform(1,5, num)
bear = 4 * np.random.uniform(2,5,num) -  .3 * lightning
deer = 7 * np.random.uniform(1,5,num) - .3 * bear - .2 * lightning
flower = 5 * np.random.uniform(5,10,num) - .4 * deer + .2 * lightning
df = pd.DataFrame({'lightning': lightning, 'bear': bear, 'deer': deer, 'flower': flower})
'''

Identify a backdoor path with one or more confounders for the relationship between deer and flowers.

There are a couple back door paths with lightning acting as the confounder in both: 

Deer <-- Bear <-- Lightning --> Flowers

Deer <-- Lightning --> Flowers

4. Draw a diagram for a situation of your own invention.  The diagram should include at least four nodes, one confounder, and one collider.  Be sure that it is acyclic (no loops).  Which node would say is most like a treatment (X)?  Which is most like an outcome (Y)?

Distance to destination affects travel time. Distance also affects average speed driven. 
The greater the distance, there is a greater chance of traffic negatively impacting travel time.
The higher the average speed driven positively impacts the travel time. Increase in traffic negatively impacts average speed.

            Distance ----> Traffic
                |  \        /
                |   \      /
                |    v    v
                |    Speed
                |      / 
                |     /  
                v    v 
            Travel Time

Treatment: Distance

Outcome: Travel Time

Homework Reflection 6

1. What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference?  (Hint: think of statistics here.  Consider that only the most extreme item ends up being used to estimate the MTE.  That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect.  But there is nevertheless a problem.)
Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness.
(Please explain / justify this answer, or give a different one if you can think of one.)

The problem is that by only considering the maximum difference, we may be ignoring other untreated items that could provide valuable information about the treatment effect. This could lead to an overestimation of the MTE if the maximum difference is an outlier.

2. Propose a solution that remedies this problem and write some code that implements your solution.  It's very important here that you clearly explain what your solution will do.
Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.
(Either code this answer or choose a different one.)

This dataset used for this solution is the same one used for the Week 6 coding quiz. The 90th percentile solution is implemented below by taking the 90th percentile of the treatment effect among the untreated items. This provides a more robust estimate of the Marginal Treatment Effect by considering a broader range of effects rather than just the maximum.

'''python
from sklearn.neighbors import NearestNeighbors

# Assume df has columns: X (treatment), Y (outcome), Z (confounder)
# Separate treated and untreated
T = df[df['X'] == 1].copy()
U = df[df['X'] == 0].copy()

# Fit NearestNeighbors on Z for each group
t_nn = NearestNeighbors(n_neighbors=1).fit(U[['Z']])
u_nn = NearestNeighbors(n_neighbors=1).fit(T[['Z']])

# For each treated, find nearest untreated (counterfactual)
dist_T, idx_T = t_nn.kneighbors(T[['Z']])
T['cf_idx'] = idx_T.flatten()
T['cf_Y'] = U.iloc[T['cf_idx']]['Y'].values
T['effect'] = T['Y'] - T['cf_Y']

# For each untreated, find nearest treated (counterfactual)
dist_U, idx_U = u_nn.kneighbors(U[['Z']])
U['cf_idx'] = idx_U.flatten()
U['cf_Y'] = T.iloc[U['cf_idx']]['Y'].values
U['effect'] = U['cf_Y'] - U['Y']

# Average Treatment Effect (ATE): average of all effects (both directions)
ate = pd.concat([T['effect'], U['effect']]).mean()

# Average Treatment Effect on the Treated (ATT): average effect for treated
att = T['effect'].mean()

# Average Treatment Effect on the Untreated (ATU): average effect for untreated
atu = U['effect'].mean()

# Marginal Treatment Effect: maximum effect among untreated and 90th percentile among untreated
mte = U['effect'].max()
mte_90th_percent = U['effect'].quantile(0.9)

print(f"ATE: {ate:.3f}")
print(f"ATT: {att:.3f}")
print(f"ATU: {atu:.3f}")
print(f"Marginal Treatment Effect: {mte:.3f}")
print(f"90th Percentile of Marginal Treatment Effect: {mte_90th_percent:.3f}")
'''

Homework Reflection 7

1. Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between $$X$$ and $$Y$$ is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen.

    C = np.random.normal(0, 1, n_obs)
    X = 2 + 0.8 * C + np.random.normal(0, 0.5, n_obs)
    Y = 1 + 1.2 * X + 0.6 * C + np.random.normal(0, 0.5, n_obs)

Above are the values chosen for the confounder C, X, and Y. The correlation between X and Y is overestimated when the confounder is left out of the model. 
This is because the confounder C positively affects both X and Y, leading to a spurious correlation between them when C is not included. 
The model without the confounder will attribute the effect of C to the relationship between X and Y, thus inflating the estimated correlation.

2. Perform a linear regression analysis in which one of the coefficients is zero, e.g.

W = [noise]
X = [noise]
Y = 2 * X + [noise]

And compute the p-value of a coefficient - in this case, the coefficient of W.  
(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)
If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)
Run the analysis 1000 times and report the best (smallest) p-value.  
If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?

No, just because the p-value is small doesn't mean the coefficient is non-zero. The issue with repeating the analysis is that it can lead to a Type 1 error, where we incorrectly reject the null hypothesis simply due to random chance. The more we run the analysis, the higher chance we have of finding a smaller p-value.

Homework Reflection 8

Include the code you used to solve the two coding quiz problems and write about the obstacles / challenges / insights you encountered while solving them.

The first problem invloved finding the ATE from File 8.1 using inverse probability weighting. Most obstacles faced were code troubleshooting and debugging, related to resizing the confounder dataframe for fitting to the model. Otherwise, the problem was straightforward.

'''python
# Find the average treatment effect with inverse probability weighting
def average_treatment_effect(df, treatment_col, outcome_col, confounder_col):
       # Initialize the logistic regression model
       model = LogisticRegression()

       # reshape the dataframe to fit the model
       Z = df[confounder_col].values.reshape(-1,1)
       model.fit(Z, df[treatment_col])

       # Calculate the propensity score
       df['propensity_score'] = model.predict_proba(Z)[:, 1]
       # Calculate the weights
       df['weights'] = np.where(df[treatment_col] == 1, 1 / df['propensity_score'], 1 / (1 - df['propensity_score']))

       # Calculate the weighted means
       treated = df[df[treatment_col] == 1]
       control = df[df[treatment_col] == 0]

       # Calculate the ATE
       treated_mean = (treated[outcome_col] * treated['weights']).sum() / treated['weights'].sum()
       control_mean = (control[outcome_col] * control['weights']).sum() / control['weights'].sum()
       ate = treated_mean - control_mean

       print(f"ATE: {ate}, Treated Mean: {treated_mean}, Control Mean: {control_mean}")
       return df, ate, treated, control
'''

The second problem involved finding the ATE from File 8.2 through matching treated units to control units using the Mahalanobis Distance. The Mahalanobis distance is a measure of the distance between a point and a distribution, which is useful for matching treated units to control units. This question was the more challenging of the two as more research and troubleshooting was required. Finding the ATE was similar to the first problem, but the matching process required more understanding of the distance matching in the inverse covariance matrix.
The second part of analyzing File 8.2 asked for the nearest Z1 and Z2 values of the treated item with the least common support, which means finding the control unit that is closest in terms of the covariates. After consulting with the LF's on Yellowdig, the question might have been phrased incorrectly, as the nearest covariates were not necessarily the nearest treated unit. Instead, the question was asking for the nearest Z1 and Z2 of a treated unit that was furthest away from a control unit. This was a bit confusing, but after some clarification, it made more sense.

'''python
def match_treated_to_control(df):
    from scipy.spatial.distance import cdist

    # Separate treated and untreated groups
    treated = df[df['X'] == 1].copy()
    control = df[df['X'] == 0].copy()

    # Create a matrix of the confounders
    Z = df[['Z1', 'Z2']].values

    # Calculate the covariance matrix of the confounders
    cov_matrix = np.cov(Z.T)
    inv_cov_matrix = np.linalg.inv(cov_matrix)

    # Calculate the Mahalanobis distance matrix
    # Rows = treated units, Columns = control units
    distance_matrix = cdist(treated[['Z1', 'Z2']], control[['Z1', 'Z2']], 
                           metric='mahalanobis', VI=inv_cov_matrix)

    # Find the closest control for each treated unit
    closest_control_indices = np.argmin(distance_matrix, axis=1)
    
    # Get the minimum distance for each treated unit to its nearest control
    min_distances_to_controls = np.min(distance_matrix, axis=1)
    
    # Find the treated unit with the MAXIMUM minimum distance (least common support)
    farthest_treated_index = np.argmax(min_distances_to_controls)
    farthest_distance = min_distances_to_controls[farthest_treated_index]
    
    # Get the treated unit with least common support
    treated_reset = treated.reset_index(drop=True)
    farthest_treated_unit = treated_reset.iloc[farthest_treated_index]

    # Find the specific control unit closest to this farthest treated unit
    closest_control_to_farthest = closest_control_indices[farthest_treated_index]
    control_reset = control.reset_index(drop=True)
    nearest_control_unit = control_reset.iloc[closest_control_to_farthest]

    # Get Y values of matched controls for ALL treated units
    treated_reset['matched_control_Y'] = control_reset.iloc[closest_control_indices]['Y'].values
    treated_reset['mahalanobis_distance'] = min_distances_to_controls

    # Calculate the ATE
    ate = (treated_reset['Y'] - treated_reset['matched_control_Y']).mean()

    print(f"ATE: {ate:.4f}")
    print(f"\nTreated unit with least common support (farthest from any control):")
    print(f"  Treated Index: {farthest_treated_index}")
    print(f"  Z1: {farthest_treated_unit['Z1']:.4f}")
    print(f"  Z2: {farthest_treated_unit['Z2']:.4f}")
    print(f"  Y: {farthest_treated_unit['Y']:.4f}")
    print(f"  Distance to nearest control: {farthest_distance:.4f}")
    print()
    print(f"Its nearest control unit:")
    print(f"  Control Index: {closest_control_to_farthest}")
    print(f"  Z1: {nearest_control_unit['Z1']:.4f}")
    print(f"  Z2: {nearest_control_unit['Z2']:.4f}")
    print(f"  Y: {nearest_control_unit['Y']:.4f}")
    
    return treated_reset, ate, farthest_treated_unit, nearest_control_unit, farthest_distance
'''