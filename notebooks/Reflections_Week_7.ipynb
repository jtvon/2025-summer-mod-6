{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a27b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cf1bb",
   "metadata": {},
   "source": [
    "Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between X and Y is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7d113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFOUNDER ANALYSIS RESULTS\n",
      "==================================================\n",
      "Sample size: 1000\n",
      "\n",
      "TRUE DATA GENERATING PROCESS:\n",
      "C ~ N(0, 1)  [Confounder]\n",
      "X = 2 + 0.8*C + error\n",
      "Y = 1 + 1.2*X + 0.6*C + error\n",
      "\n",
      "KEY STATISTICS:\n",
      "True correlation between X and Y: 0.9416\n",
      "Correlation between X and C: 0.8481\n",
      "Correlation between Y and C: 0.8947\n",
      "\n",
      "MODEL RESULTS:\n",
      "------------------------------\n",
      "CORRECT MODEL: Y ~ X + C\n",
      "  Intercept: 0.9772\n",
      "  X coefficient: 1.1982 (True: 1.2)\n",
      "  C coefficient: 0.5832 (True: 0.6)\n",
      "  R-squared: 0.9195\n",
      "\n",
      "BIASED MODEL: Y ~ X (omits C)\n",
      "  Intercept: -0.1028\n",
      "  X coefficient: 1.7328\n",
      "  R-squared: 0.8866\n",
      "\n",
      "CORRELATION ANALYSIS:\n",
      "------------------------------\n",
      "True correlation X-Y: 0.9416\n",
      "Correlation implied by biased model: 0.9416\n",
      "\n",
      "CONCLUSION: The correlation is CORRECTLY ESTIMATED\n",
      "Bias amount: 0.0000\n",
      "Bias percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Create a linear regression model with a confounder (C) that is left out\n",
    "np.random.seed(0)\n",
    "n_obs = 1000\n",
    "\n",
    "# C is the confounder\n",
    "C = np.random.normal(0, 1, n_obs)\n",
    "X = 2 + 0.8 * C + np.random.normal(0, 0.5, n_obs)\n",
    "Y = 1 + 1.2 * X + 0.6 * C + np.random.normal(0, 0.5, n_obs)\n",
    "\n",
    "# Calculate TRUE correlation between X and Y\n",
    "true_correlation_XY = np.corrcoef(X, Y)[0, 1]\n",
    "\n",
    "# Model 1: Correct model Y ~ X + C (includes confounder)\n",
    "model_with_confounder = LinearRegression()\n",
    "X_with_C = np.column_stack([X, C])\n",
    "model_with_confounder.fit(X_with_C, Y)\n",
    "\n",
    "# Model 2: Incorrect model Y ~ X (omits confounder C)\n",
    "model_without_confounder = LinearRegression()\n",
    "model_without_confounder.fit(X.reshape(-1, 1), Y)\n",
    "\n",
    "# Calculate R-squared for both models\n",
    "r2_with_confounder = model_with_confounder.score(X_with_C, Y)\n",
    "r2_without_confounder = model_without_confounder.score(X.reshape(-1, 1), Y)\n",
    "\n",
    "# The correlation from the bivariate regression (without confounder) \n",
    "# is approximately sqrt(R-squared) when there's a positive relationship\n",
    "correlation_from_bivariate = np.sqrt(r2_without_confounder) * np.sign(model_without_confounder.coef_[0])\n",
    "\n",
    "print(\"CONFOUNDER ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sample size: {n_obs}\")\n",
    "print()\n",
    "print(\"TRUE DATA GENERATING PROCESS:\")\n",
    "print(\"C ~ N(0, 1)  [Confounder]\")\n",
    "print(\"X = 2 + 0.8*C + error\")\n",
    "print(\"Y = 1 + 1.2*X + 0.6*C + error\")\n",
    "print()\n",
    "print(\"KEY STATISTICS:\")\n",
    "print(f\"True correlation between X and Y: {true_correlation_XY:.4f}\")\n",
    "print(f\"Correlation between X and C: {np.corrcoef(X, C)[0,1]:.4f}\")\n",
    "print(f\"Correlation between Y and C: {np.corrcoef(Y, C)[0,1]:.4f}\")\n",
    "print()\n",
    "print(\"MODEL RESULTS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"CORRECT MODEL: Y ~ X + C\")\n",
    "print(f\"  Intercept: {model_with_confounder.intercept_:.4f}\")\n",
    "print(f\"  X coefficient: {model_with_confounder.coef_[0]:.4f} (True: 1.2)\")\n",
    "print(f\"  C coefficient: {model_with_confounder.coef_[1]:.4f} (True: 0.6)\")\n",
    "print(f\"  R-squared: {r2_with_confounder:.4f}\")\n",
    "print()\n",
    "print(\"BIASED MODEL: Y ~ X (omits C)\")\n",
    "print(f\"  Intercept: {model_without_confounder.intercept_:.4f}\")\n",
    "print(f\"  X coefficient: {model_without_confounder.coef_[0]:.4f}\")\n",
    "print(f\"  R-squared: {r2_without_confounder:.4f}\")\n",
    "print()\n",
    "print(\"CORRELATION ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"True correlation X-Y: {true_correlation_XY:.4f}\")\n",
    "print(f\"Correlation implied by biased model: {correlation_from_bivariate:.4f}\")\n",
    "print()\n",
    "\n",
    "# Determine if correlation is over or underestimated\n",
    "if abs(correlation_from_bivariate) > abs(true_correlation_XY):\n",
    "    bias_direction = \"OVERESTIMATED\"\n",
    "elif abs(correlation_from_bivariate) < abs(true_correlation_XY):\n",
    "    bias_direction = \"UNDERESTIMATED\"\n",
    "else:\n",
    "    bias_direction = \"CORRECTLY ESTIMATED\"\n",
    "\n",
    "print(f\"CONCLUSION: The correlation is {bias_direction}\")\n",
    "print(f\"Bias amount: {correlation_from_bivariate - true_correlation_XY:.4f}\")\n",
    "print(f\"Bias percentage: {((correlation_from_bivariate - true_correlation_XY)/true_correlation_XY)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97c38e",
   "metadata": {},
   "source": [
    "Perform a linear regression analysis in which one of the coefficients is zero, e.g.\n",
    "\n",
    "W = [noise]\\\n",
    "X = [noise]\\\n",
    "Y = 2 * X + [noise]\n",
    "\n",
    "And compute the p-value of a coefficient - in this case, the coefficient of W.  \n",
    "(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)\n",
    "If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)\n",
    "Run the analysis 1000 times and report the best (smallest) p-value.  \n",
    "If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a3f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.593274</td>\n",
       "      <td>1.593274</td>\n",
       "      <td>4.779821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.568722</td>\n",
       "      <td>0.568722</td>\n",
       "      <td>1.706167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.114487</td>\n",
       "      <td>-0.114487</td>\n",
       "      <td>-0.343461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.251630</td>\n",
       "      <td>0.251630</td>\n",
       "      <td>0.754891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.210856</td>\n",
       "      <td>-1.210856</td>\n",
       "      <td>-3.632567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          W         X         Y\n",
       "0  1.593274  1.593274  4.779821\n",
       "1  0.568722  0.568722  1.706167\n",
       "2 -0.114487 -0.114487 -0.343461\n",
       "3  0.251630  0.251630  0.754891\n",
       "4 -1.210856 -1.210856 -3.632567"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new dataset with INDEPENDENT noise variables\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W_2 = np.random.normal(0, 1, n_obs)  # W is independent noise\n",
    "X_2 = np.random.normal(0, 1, n_obs)  # X is independent noise  \n",
    "Y_2 = 2 * X_2 + np.random.normal(0, 1, n_obs)  # Y depends only on X, plus independent noise\n",
    "\n",
    "# Combine the data into a DataFrame\n",
    "df = pd.DataFrame({\"W\":W_2, \"X\":X_2, \"Y\":Y_2})\n",
    "print(\"Data generation complete:\")\n",
    "print(f\"Correlation between W and X: {np.corrcoef(W_2, X_2)[0,1]:.4f}\")\n",
    "print(f\"Correlation between W and Y: {np.corrcoef(W_2, Y_2)[0,1]:.4f}\")\n",
    "print(f\"Correlation between X and Y: {np.corrcoef(X_2, Y_2)[0,1]:.4f}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a3f62",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLinAlgError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m X_with_intercept = np.column_stack([np.ones(n), X_model])\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Calculate covariance matrix: (X'X)^(-1) * MSE\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m XtX_inv = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_with_intercept\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_with_intercept\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m mse = rss / dof\n\u001b[32m     28\u001b[39m cov_matrix = mse * XtX_inv\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Masters/.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.py:609\u001b[39m, in \u001b[36minv\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m    606\u001b[39m signature = \u001b[33m'\u001b[39m\u001b[33mD->D\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33md->d\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call=_raise_linalgerror_singular, invalid=\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    608\u001b[39m               over=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, under=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     ainv = \u001b[43m_umath_linalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv.astype(result_t, copy=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Masters/.venv/lib/python3.13/site-packages/numpy/linalg/_linalg.py:104\u001b[39m, in \u001b[36m_raise_linalgerror_singular\u001b[39m\u001b[34m(err, flag)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33m\"\u001b[39m\u001b[33mSingular matrix\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mLinAlgError\u001b[39m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Fit a linear regression model and compute the p_value of the coefficient of W\n",
    "model = LinearRegression()\n",
    "X_model = df[[\"X\",\"W\"]]\n",
    "model.fit(X_model, df[\"Y\"])\n",
    "\n",
    "# Extract coefficients and p-values\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Calculate the p-values using t-statistic\n",
    "from scipy import stats\n",
    "n = len(df)\n",
    "k = X_model.shape[1]  # number of predictors\n",
    "dof = n - k - 1  # degrees of freedom\n",
    "\n",
    "# Calculate residuals and residual sum of squares\n",
    "y_pred = model.predict(X_model)\n",
    "residuals = df[\"Y\"] - y_pred\n",
    "rss = np.sum(residuals**2)\n",
    "\n",
    "# Calculate standard error of coefficients\n",
    "# Design matrix with intercept column\n",
    "X_with_intercept = np.column_stack([np.ones(n), X_model])\n",
    "\n",
    "# Calculate covariance matrix: (X'X)^(-1) * MSE\n",
    "try:\n",
    "    XtX_inv = np.linalg.inv(X_with_intercept.T @ X_with_intercept)\n",
    "    cov_matrix = mse * XtX_inv\n",
    "    \n",
    "    # Standard errors are square root of diagonal elements\n",
    "    std_errors = np.sqrt(np.diag(cov_matrix))\n",
    "    \n",
    "    # Calculate t-statistics\n",
    "    t_stats = np.concatenate([[intercept], coefficients]) / std_errors\n",
    "    \n",
    "    # Calculate p-values (two-tailed test)\n",
    "    p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), dof))\n",
    "    \n",
    "    print(\"SINGLE REGRESSION ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Sample size: {n}\")\n",
    "    print(f\"Degrees of freedom: {dof}\")\n",
    "    print()\n",
    "    print(\"COEFFICIENT RESULTS:\")\n",
    "    print(f\"Intercept: {intercept:.4f} (t={t_stats[0]:.3f}, p={p_values[0]:.6f})\")\n",
    "    print(f\"X coefficient: {coefficients[0]:.4f} (t={t_stats[1]:.3f}, p={p_values[1]:.6f})\")\n",
    "    print(f\"W coefficient: {coefficients[1]:.4f} (t={t_stats[2]:.3f}, p={p_values[2]:.6f})\")\n",
    "    print()\n",
    "    print(f\"P-value for W coefficient: {p_values[2]:.6f}\")\n",
    "    print(f\"Is W significant at α=0.05? {'Yes' if p_values[2] < 0.05 else 'No'}\")\n",
    "    print()\n",
    "    print(\"Note: W should have NO effect (true coefficient = 0)\")\n",
    "    print(\"If p < 0.05, this would be a Type I error (false positive)\")\n",
    "    \n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(f\"Matrix inversion error: {e}\")\n",
    "    print(\"This usually occurs when predictors are perfectly correlated.\")\n",
    "    print(\"Check that W and X are independent variables.\")\n",
    "    print(f\"Correlation between W and X: {np.corrcoef(df['W'], df['X'])[0,1]:.6f}\")\n",
    "    print(\"Solution: Generate independent noise variables for W and X.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e85bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis 1000 times and find the smallest p-value\n",
    "def calculate_p_value_for_w(n_obs=1000):\n",
    "    \"\"\"\n",
    "    Generate data where W has zero effect and calculate p-value for W coefficient\n",
    "    \"\"\"\n",
    "    # Generate independent noise variables\n",
    "    W = np.random.normal(0, 1, n_obs)  # W is just noise\n",
    "    X = np.random.normal(0, 1, n_obs)  # X is independent noise\n",
    "    Y = 2 * X + np.random.normal(0, 1, n_obs)  # Y depends only on X, not W\n",
    "    \n",
    "    # Fit model: Y ~ X + W (but W should have zero effect)\n",
    "    model = LinearRegression()\n",
    "    X_model = np.column_stack([X, W])\n",
    "    model.fit(X_model, Y)\n",
    "    \n",
    "    # Calculate p-value for W coefficient\n",
    "    n = len(Y)\n",
    "    k = 2  # number of predictors (X and W)\n",
    "    dof = n - k - 1\n",
    "    \n",
    "    # Calculate residuals and MSE\n",
    "    y_pred = model.predict(X_model)\n",
    "    residuals = Y - y_pred\n",
    "    mse = np.sum(residuals**2) / dof\n",
    "    \n",
    "    # Calculate standard errors\n",
    "    X_with_intercept = np.column_stack([np.ones(n), X_model])\n",
    "    try:\n",
    "        XtX_inv = np.linalg.inv(X_with_intercept.T @ X_with_intercept)\n",
    "        std_errors = np.sqrt(np.diag(mse * XtX_inv))\n",
    "        \n",
    "        # Calculate t-statistic and p-value for W (index 2)\n",
    "        t_stat_w = model.coef_[1] / std_errors[2]  # W coefficient / its standard error\n",
    "        p_value_w = 2 * (1 - stats.t.cdf(np.abs(t_stat_w), dof))\n",
    "        \n",
    "        return p_value_w, model.coef_[1]\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Return NaN if matrix is singular\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# Run simulation 1000 times\n",
    "print(\"SIMULATION: 1000 TRIALS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"True model: Y = 2*X + noise\")\n",
    "print(\"Fitted model: Y ~ X + W (W should have zero effect)\")\n",
    "print()\n",
    "\n",
    "n_trials = 1000\n",
    "p_values_w = []\n",
    "coefficients_w = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    p_val, coef = calculate_p_value_for_w(n_obs=1000)\n",
    "    # Only include valid results (not NaN)\n",
    "    if not np.isnan(p_val):\n",
    "        p_values_w.append(p_val)\n",
    "        coefficients_w.append(coef)\n",
    "\n",
    "p_values_w = np.array(p_values_w)\n",
    "coefficients_w = np.array(coefficients_w)\n",
    "\n",
    "print(f\"Successfully completed {len(p_values_w)} out of {n_trials} trials\")\n",
    "if len(p_values_w) < n_trials:\n",
    "    print(f\"Excluded {n_trials - len(p_values_w)} trials due to matrix singularity\")\n",
    "\n",
    "# Find the smallest p-value\n",
    "min_p_value = np.min(p_values_w)\n",
    "min_p_index = np.argmin(p_values_w)\n",
    "\n",
    "# Count how many times we get p < 0.05 (false positives)\n",
    "false_positives = np.sum(p_values_w < 0.05)\n",
    "false_positive_rate = false_positives / n_trials\n",
    "\n",
    "print(\"SIMULATION RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of trials: {n_trials}\")\n",
    "print(f\"Smallest p-value: {min_p_value:.8f}\")\n",
    "print(f\"Coefficient value for smallest p-value: {coefficients_w[min_p_index]:.4f}\")\n",
    "print(f\"Times p < 0.05: {false_positives} out of {n_trials}\")\n",
    "print(f\"False positive rate: {false_positive_rate:.3f} ({false_positive_rate*100:.1f}%)\")\n",
    "print()\n",
    "print(\"THEORETICAL EXPECTATION:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Expected false positive rate: 0.05 (5%)\")\n",
    "print(f\"Expected number of false positives: {0.05 * n_trials:.0f}\")\n",
    "print(f\"Observed vs Expected: {false_positive_rate:.3f} vs 0.05\")\n",
    "print()\n",
    "\n",
    "# Analysis of the smallest p-value\n",
    "print(\"ANALYSIS OF THE SMALLEST P-VALUE:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"The smallest p-value is: {min_p_value:.8f}\")\n",
    "print(f\"This corresponds to a W coefficient of: {coefficients_w[min_p_index]:.4f}\")\n",
    "print()\n",
    "if min_p_value < 0.05:\n",
    "    print(\"✗ The smallest p-value IS less than 0.05\")\n",
    "    print(\"✗ This does NOT mean W actually has a nonzero effect\")\n",
    "    print(\"✗ This is a Type I error (false positive)\")\n",
    "else:\n",
    "    print(\"✓ The smallest p-value is not less than 0.05\")\n",
    "    print(\"✓ We correctly fail to reject the null hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ba770",
   "metadata": {},
   "source": [
    "## The Multiple Testing Problem\n",
    "\n",
    "**Key Questions Answered:**\n",
    "\n",
    "1. **If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?**\n",
    "   - **NO!** Even if we find p < 0.05, this does NOT mean W actually has a nonzero effect\n",
    "   - We know W has zero effect by construction (it's just noise)\n",
    "   - A p-value < 0.05 in this case is a **Type I error** (false positive)\n",
    "\n",
    "2. **What is the problem with repeating the analysis?**\n",
    "   - **Multiple Testing Problem**: The more tests we run, the higher the chance of finding at least one \"significant\" result by pure chance\n",
    "   - With 1000 trials at α = 0.05, we expect about 50 false positives (5% of 1000)\n",
    "   - The probability of finding at least one p < 0.05 in 1000 trials is approximately: 1 - (0.95)^1000 ≈ 100%\n",
    "   - This is called \"p-hacking\" or \"data dredging\" when done intentionally\n",
    "\n",
    "**Why This Matters:**\n",
    "- Publishing only the \"best\" result from multiple trials is misleading\n",
    "- Need multiple testing corrections (Bonferroni, FDR, etc.)\n",
    "- Emphasizes importance of replication and preregistration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of p-values and demonstrate multiple testing correction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Histogram of p-values\n",
    "ax1.hist(p_values_w, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(0.05, color='red', linestyle='--', linewidth=2, label='α = 0.05')\n",
    "ax1.axvline(min_p_value, color='orange', linestyle='--', linewidth=2, \n",
    "           label=f'Min p = {min_p_value:.6f}')\n",
    "ax1.set_xlabel('P-value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of P-values for W Coefficient\\n(1000 trials, true effect = 0)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add text showing false positive count\n",
    "ax1.text(0.6, ax1.get_ylim()[1]*0.8, f'False positives: {false_positives}\\nRate: {false_positive_rate:.1%}', \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "# Plot 2: Scatter plot of coefficients vs p-values\n",
    "ax2.scatter(coefficients_w, p_values_w, alpha=0.6, s=20, color='blue')\n",
    "ax2.axhline(0.05, color='red', linestyle='--', linewidth=2, label='α = 0.05')\n",
    "ax2.axvline(0, color='green', linestyle='-', linewidth=2, label='True effect = 0')\n",
    "ax2.scatter(coefficients_w[min_p_index], min_p_value, color='orange', s=100, \n",
    "           label=f'Min p-value', zorder=5)\n",
    "\n",
    "# Add Bonferroni correction line\n",
    "bonferroni_alpha = 0.05 / n_trials\n",
    "ax2.axhline(bonferroni_alpha, color='purple', linestyle=':', linewidth=2, \n",
    "           label=f'Bonferroni α = {bonferroni_alpha:.6f}')\n",
    "\n",
    "ax2.set_xlabel('W Coefficient Estimate')\n",
    "ax2.set_ylabel('P-value')\n",
    "ax2.set_title('W Coefficient vs P-value\\n(with multiple testing correction)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate multiple testing corrections\n",
    "bonferroni_significant = np.sum(p_values_w < (0.05 / n_trials))\n",
    "print()\n",
    "print(\"MULTIPLE TESTING CORRECTIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original α level: 0.05\")\n",
    "print(f\"Bonferroni corrected α: {0.05/n_trials:.6f}\")\n",
    "print(f\"Significant results with Bonferroni correction: {bonferroni_significant}\")\n",
    "print()\n",
    "print(\"PROBABILITY CALCULATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Probability of at least one false positive in {n_trials} trials:\")\n",
    "prob_at_least_one = 1 - (0.95)**n_trials\n",
    "print(f\"P(at least one p < 0.05) = 1 - (0.95)^{n_trials} = {prob_at_least_one:.6f}\")\n",
    "print(f\"This is essentially 100% - we're almost guaranteed to find a 'significant' result!\")\n",
    "print()\n",
    "print(\"LESSON: This demonstrates why multiple testing correction is crucial\")\n",
    "print(\"and why 'p-hacking' (cherry-picking the best result) is problematic.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
