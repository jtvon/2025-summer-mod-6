Homework reflection 1

1. In Coding Quiz 1, you are asked to find the distance of the farthest match in a set.  Is this farthest match distance too far to be a meaningful match?  How can you decide this?

For the context of this problem given the data, the match is still meaningful. When looking at a histogram distribution of the y value differences, there seemed to be an almost bimodal distribution. This infers there is a confounding variable not accounted for in the analysis.

2. In Coding Quiz 1, there are two approaches to matching: 
(A) Picking the best match X = 0 corresponding to each X = 1 using Z values.
(B) Using radius_neighbors to pick all matches X = 0 within a distance or 0.2 of each X = 1.

Invent your own type of matching similar to 1 and 2 (or look one up on the internet), which has a different way to pick the matches in X = 0.  Clearly explain the approach you invented or found.

Coarsened Exact Matching (CEM) is an effective method at discarding unmatched units and yielding a greater exact match through strict conditions. It does this by binning covariates before performing exact match. Where exact matching fails on higher dimensional data, the coarsening (binning) allows continuous variables to be grouped together before being matched. The CEM method is much simpler since it has clear definitions, but is likely to perform worse than KNeighbors or Radius matching.

Homework reflection 2

1. Invent an example situation that would use fixed effects.

An example that comes to mind would be finding what types of engines are more fuel efficient for vehicles. Variables that can effect fuel economy would be weight, type of engine, and transmission type. Automatic and Manual vehicles can have very different effects since they have different gear ratio's. Driving style also plays an impact, one might say that vehicles with manual transmission tend to be driven more aggressively. Running Fixed Effect regression on a dataset split out by transmission type will give you a more accurate representation of fuel economy. Compared to a single equation for all.

2. Write a Python program that performs a bootstrap simulation to find the variance in the mean of the Pareto distribution when different samples are taken.  Explain what you had to do for this.  As you make the full sample size bigger (for the same distribution), what happens to the variance of the mean of the samples?  Does it stay about the same, get smaller, or get bigger?

A Pareto distribution is a probability distribution often used to model phenomena where a small number of occurences account for the majority of the effect, such as wealth. Programming the distribution was fairly easy since numpy as a function that synthesizes a pareto distibution, all that's needed is the size and shape of the distribution. Size being the number of samples and the shape governing the fatness of the tail, requiring the value be larger than 0. The bootstrap simulation selects a value from the pareto distribution and replaces it. This allows the random samples to be selected at equal probability. Given the number of simulations to run, the mean is taken and stored for calculation of the variance. As the sample size increases, the variance in sample means decreases. 

Homework reflection 3

1. In the event study in Coding Quiz 3, how would we go about testing for a change in the second derivative as well?

We would need to account for a change in slope over time in our equation. This can be done by adding a quadratic term to the regression equation called B^2. This term allows us to track the slope curvature before and after the event. If the coefficient is significanlyt different from zero, then we can conclude that the second derivative has changed.

2. Create your own scenario that illustrates differences-in-differences. Describe the story behind the data and show whether there is a nonzero treatment effect.

Suppose there are two racing teams, Team A and Team B. Team A adopts a new tire design at the start of the season, while Team B continues using the old design. We collect average lap times for both teams before and after the new tire is introduced.

Before the new tire:
- Team A average lap time: 70 seconds
- Team B average lap time: 71 seconds

After the new tire:
- Team A average lap time: 68 seconds
- Team B average lap time: 70.5 seconds

The difference-in-differences estimate is:
[(68 - 70) - (70.5 - 71)] = (-2) - (-0.5) = -1.5 seconds

This suggests the new tire reduced lap times by 1.5 seconds more for Team A than any general improvement seen by Team B, indicating a nonzero treatment effect from the new tire design.

Homework reflection 4

1. The Coding Quiz gives two options for instrumental variables.  For the second item (dividing the range of W into multiple ranges), explain how you did it, show your code, and discuss any issues you encountered.

Might have sot this question wrong on the quiz, therefore decided to rewrite and re-evaluate the question. Originally I mistook the problem as a Differences in Differences problem and calculated for the effect. Re-evaluating for the instrumental variables effect allows me to see from a different perspective. For the re-evaluation, the main goal was to calculate the effect over multiple ranges of W of varying distance. This was done by establishing a range of N values that would serve as the number of bins. The effect within each bin was measured, added to a list and then averaged over the total numver of bins. The avg_effects were then plotted on a line chart to showcase the change in effect based on the number of bins.

'''Python
import matplotlib.pyplot as plt

# Try different values of N and plot the average effect for each
N_values = range(5, 100)
avg_effects = []

for N in N_values:
    w_min = df_1['W'].min()
    w_max = df_1['W'].max()
    w_edges = np.linspace(w_min, w_max, N + 1)
    effects = []

    for i in range(N):
        w_lo = w_edges[i]
        w_hi = w_edges[i + 1]
        mask = (df_1['W'] >= w_lo) & (df_1['W'] < w_hi)
        df_bin = df_1[mask]
        # Only compute if both Z=0 and Z=1 are present
        if df_bin['Z'].nunique() == 2:
            y_1 = df_bin['Y'][df_bin['Z'] == 1].mean()
            y_0 = df_bin['Y'][df_bin['Z'] == 0].mean()
            x_1 = df_bin['X'][df_bin['Z'] == 1].mean()
            x_0 = df_bin['X'][df_bin['Z'] == 0].mean()
            x_diff = x_1 - x_0
            if x_diff != 0:
                effect = (y_1 - y_0) / x_diff
                effects.append(effect)
    if effects:
        avg_effect = np.mean(effects)
    else:
        avg_effect = np.nan

    avg_effects.append(avg_effect)

# Plot the results
plt.figure(figsize=(8, 5))
plt.plot(N_values, avg_effects, marker='o')
plt.xlabel('Number of W bins (N)')
plt.ylabel('Average Effect')
plt.title('Average Effect vs. Number of W Bins')
plt.grid(True)
plt.show()
'''

2. Plot the college outcome (Y) vs. the test score (X) in a small range of test scores around 80. On the plot, compare it with the Y probability predicted by logistic regression. The ground truth Y value is 0 or 1; don't just plot 0 or 1 - that will make it unreadable.  Find some way to make it look better than that.

'''Python
import statsmodels.api as sm

for df, label in zip([df_2, df_3], ['df_2', 'df_3']):
    # Create a binary outcome for logistic regression (e.g., Y > median(Y))
    y_binary = (df['Y'] > df['Y'].median()).astype(int)
    df['running'] = df['X'] - 80
    df['treat'] = (df['X'] >= 80).astype(int)

    # Model: Y_binary ~ treat + running
    X_rdd = sm.add_constant(df[['treat', 'running']])
    model_logit = sm.Logit(y_binary, X_rdd).fit(disp=0)

    print(f"\nLogistic Regression Discontinuity Summary for {label}:")
    print(model_logit.summary())

    # Plotting in a small range around X=80
    x_window = 2
    mask = (df['X'] >= 80 - x_window) & (df['X'] <= 80 + x_window)
    df_window = df[mask].copy()
    y_window = y_binary[mask]

    plt.figure(figsize=(8, 5))

    # Plot moving average of Y (smoothed ground truth)
    df_window_sorted = df_window.sort_values('X')
    window_size = 10  # number of points for moving average
    y_smooth = df_window_sorted['Y'].rolling(window=window_size, center=True, min_periods=1).mean()
    plt.plot(df_window_sorted['X'], y_smooth, '.-', color='gray', label='Smoothed Y (moving avg)')

    # Plot predicted probability from logistic regression
    x_vals = np.linspace(80 - x_window, 80 + x_window, 200)
    running_vals = x_vals - 80
    treat_vals = (x_vals >= 80).astype(int)
    X_pred = np.column_stack([np.ones_like(x_vals), treat_vals, running_vals])
    y_pred_prob = model_logit.predict(X_pred)
    plt.plot(x_vals, y_pred_prob, color='purple', label='Logistic Regression Prediction')

    plt.axvline(80, color='red', linestyle='--', label='Cutoff at X=80')
    plt.xlabel('Test Score (X)')
    plt.ylabel('College Outcome (Y)')
    plt.title(f'College Outcome vs. Test Score near X=80 ({label})')
    plt.legend()
    plt.show()
'''
